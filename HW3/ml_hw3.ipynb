{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103590450 四資四 馬茂源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:51.676096Z",
     "start_time": "2018-04-18T07:42:51.672332Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import FastICA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:51.688929Z",
     "start_time": "2018-04-18T07:42:51.679197Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyKNeighborsClassifier:\n",
    "    \n",
    "    def __init__(self, n_neighbors=3, **kwargs):\n",
    "        self._k = n_neighbors\n",
    "        self._X = self._y = None\n",
    "        self.set_params(**kwargs)\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return self.__dict__\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._X = X.copy()\n",
    "        self._y = y.copy()\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        distances = np.apply_along_axis(lambda x1: np.linalg.norm(x-x1), \n",
    "                                        1, self._X)\n",
    "        X_candidates = np.argsort(distances)[:self._k]\n",
    "        y_candidates = self._y[X_candidates]\n",
    "        return np.argmax(np.bincount(y_candidates.astype('int64')))\n",
    "    \n",
    "    def score(self, X, y_true):\n",
    "        return accuracy_score(y_true, self.predict(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.apply_along_axis(lambda x: self._predict(x), 1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:51.698792Z",
     "start_time": "2018-04-18T07:42:51.691175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "feature_names = iris.feature_names.copy()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "print(iris_X.shape, iris_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . In  this  problem,  you  are  asked  to  use  ICA  on  Iris  dataset  for  dimensionality reduction  before classification.  To  simplify  the  problem,  you  do  not  need  to implement the ICA program. \n",
    "\n",
    "Instead, find an existing one and learn how to use it. As you may not be able to store internal parameters of the ICA, input all of the 150  samples  to  find  the  corresponding  independent  components  as  the preprocessing  step.  \n",
    "\n",
    "You  may assume  that  there  are  four  sources  and  four observations. On the obtained four components, pick the two components with largest energy as new features. Randomly pick 70 % of the samples (represented by  new features)  as  training  set  and  the  rest  as  test  set.  Implement  the  3-NN classifier  to  compute  the  accuracy. Repeat  the  drawing  and  the  3-NN classification 10 times and compute the average accuracy and accuracy variance. For simplicity, use the Euclidean distance in the k-NN computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:51.706507Z",
     "start_time": "2018-04-18T07:42:51.700939Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=2)\n",
    "ica_X = ica.fit_transform(iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:52.159270Z",
     "start_time": "2018-04-18T07:42:51.709133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg acc:0.9622222222222223, variance of acc:0.0010913580246913583\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(10):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(ica_X, \n",
    "                                                        iris_y, \n",
    "                                                        train_size=0.7)\n",
    "    model = MyKNeighborsClassifier()\n",
    "    model.fit(train_X, train_y)\n",
    "    acc.append(model.score(test_X, test_y))\n",
    "print('avg acc:{}, variance of acc:{}'.format(np.mean(acc), np.var(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 . We learned the k-means for clustering in the lecture. Implement the algorithm with the Iris dataset.\n",
    "\n",
    "In this problem, we know k = 3. Use the first sample in each class as the initial cluster center to do the clustering. \n",
    "\n",
    "Remember that the cluster centers are points in 4-dimensional space. To have a unique answer, use the same sequence given in the dataset to feed into your program. \n",
    "\n",
    "That is, do not shuffler the dataset. Once your program converges,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:52.168742Z",
     "start_time": "2018-04-18T07:42:52.161664Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyKmeans:\n",
    "    \n",
    "    def __init__(self, n_clusters=3, init=None, max_iter=300, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroids = init.copy()\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def _update_centroids(self, X, y):\n",
    "        for i, c in enumerate(np.unique(y)):\n",
    "            new_center = np.mean(X[y==c, :], axis=0)\n",
    "            self.centroids[i] = new_center\n",
    "            \n",
    "    def _predict(self, x):\n",
    "        return np.argmin(np.linalg.norm(self.centroids-x, axis=1))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.apply_along_axis(lambda x: self._predict(x), 1, X)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        if self.centroids == 'random' or self.centroids is None:\n",
    "            idx = np.random.randint(X.shape[0], size=self.n_clusters)\n",
    "            self.centroids = X[idx, :]\n",
    "            # print(self.centroids)\n",
    "          \n",
    "        for i in range(self.max_iter):\n",
    "            y = self.predict(X)\n",
    "            previous_centroids = self.centroids.copy()\n",
    "            self._update_centroids(X, y)\n",
    "            \n",
    "            #print(np.linalg.norm(previous_centroids-self.centroids, axis=1) < self.tol) \n",
    "            if np.all(np.linalg.norm(previous_centroids-self.centroids, axis=1) < self.tol):\n",
    "                print('convergence at iteration-{}'.format(i))\n",
    "                break\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:42:52.177315Z",
     "start_time": "2018-04-18T07:42:52.171089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.3, 3.3, 6. , 2.5]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = iris_X[[0, 50, 100], :]\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:44:41.315282Z",
     "start_time": "2018-04-18T07:44:41.293928Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence at iteration-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:21: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "model = MyKmeans(init=init).fit(iris_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) print out the coordinates of the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:44:42.992463Z",
     "start_time": "2018-04-18T07:44:42.986268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.006     , 3.418     , 1.464     , 0.244     ],\n",
       "       [5.9016129 , 2.7483871 , 4.39354839, 1.43387097],\n",
       "       [6.85      , 3.07368421, 5.74210526, 2.07105263]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) and the number of members (sample points) in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:44:45.409111Z",
     "start_time": "2018-04-18T07:44:45.400129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 62, 38])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(model.predict(iris_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) According to the labels of data samples, how many of them are placed in wrong clusters? Use a majority vote to determine the label of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T07:44:47.069612Z",
     "start_time": "2018-04-18T07:44:47.059889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8933333333333333"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(iris_y, model.predict(iris_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . We know that the GMM can be viewed as a “soft” clustering method. To simplify the difficulty level, we will implement the univariate GMM. Use the third feature (petal length) as the input to your GMM. \n",
    "\n",
    "The settings are three Gaussians with the following initial values: $\\mu_{1}$= 1, $\\mu_{2}$= 4, $\\mu_{3}$= 6, $\\sigma_{1}^{2}$= $\\sigma_{2}^{2}$= $\\sigma_{3}^{2}$= 1, $a_{1}$= 0.5, $a_{2}$= $a_{3}$= 0.25. \n",
    "\n",
    "To have a unique answer, iterate the EM steps 3,000 times (epochs). \n",
    "\n",
    "a. Print out the GMM parameters. \n",
    "\n",
    "b. If you want to convert the “soft” clustering results to “hard” clustering ones, how do you do it? \n",
    "\n",
    "c. Use your method in (b) to find the number of members in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
