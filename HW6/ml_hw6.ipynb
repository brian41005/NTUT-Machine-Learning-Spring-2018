{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103590450 四資四 馬茂源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:35:59.133345Z",
     "start_time": "2018-06-18T16:35:59.131340Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 .\tWe mentioned that the parameters $\\gamma$ and $\\beta$ in batch normalization are trained through backprop. Explain why we don’t want to directly compute these two parameters from the training samples and also give the details how the training is carried out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ and $\\beta$是根據model training時在經過batch norm後所學習的數值跟input沒有關係，$\\gamma$, $\\beta$目的是為了反轉換來學習$\\mu$, $\\sigma$的影響力，如果$\\gamma$, $\\beta$跟$\\mu$, $\\sigma$一樣(抵銷)就是代表$\\mu$, $\\sigma$沒有影響力，$\\gamma$ ,$\\beta$是經由model學習出來的，我們在訓練之前無從得知這兩參數實際的數值。\n",
    "\n",
    "學習時候神經元輸出經過batch norm轉換再經過$\\gamma$ and $\\beta$反轉換一次最後丟入activation func，backprop修正時先從activation func開始接著$\\gamma$ and $\\beta$之後W參數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 .\tWe use the “Reparameterization Trick” in training Variational AutoEncoder (VAE). Explain why this trick is necessary and how it is accomplished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為在VAE，如果要讓model可以無中生有，中間的latent space的分布很重要，但一般來說train set mapping不到完整的space，因此一個單純的AE在被訓練完成後，通常會一直產生之前train set的東西，無法無中生有是因為train set很少涵蓋整個feature space因此latent space不會是continuous,\n",
    "這對分類是好事但對generation可不是好現象。我們希望decoder是吃 random sampling出來的東西，但神經網路**backpropogation cannot flow through random node**，若要克服此問題可以使用reparameterization trick，我們以$\\mu$, $\\sigma$來代替我們學習normal distribution特性，$\\mu$, $\\sigma$ 由encoder部分學習，decoder負責解開sampling，$\\mu$代表dim的方向而$\\sigma$分布大小，所以decoder不再是從單一point來學習而是從機率分布來看latent feature，即使decoder只看過某些點，但這些點周圍的區域依然可以refer出來差不多的東西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[以keras為例](https://gist.github.com/irhumshafkat/11009f23950e31c6612402b80fae6596)\n",
    "```python\n",
    "# build your encoder upto here. It can simply be a series of dense layers, a convolutional network\n",
    "# or even an LSTM decoder. Once made, flatten out the final layer of the encoder, call it hidden.\n",
    "\n",
    "# we use Keras to build the graph\n",
    "\n",
    "latent_size = 5\n",
    "mean = Dense(latent_size)(hidden)\n",
    "\n",
    "# we usually don't directly compute the stddev σ \n",
    "# but the log of the stddev instead, which is log(σ)\n",
    "# the reasoning is similar to why we use softmax, instead of directly outputting\n",
    "# numbers in fixed range [0, 1], the network can output a wider range of numbers which we can later compress down\n",
    "log_stddev = Dense(latent_size)(hidden)\n",
    "\n",
    "def sampler(mean, log_stddev):\n",
    "    # we sample from the standard normal a matrix of batch_size * latent_size (taking into account minibatches)\n",
    "    std_norm = K.random_normal(shape=(K.shape(mean)[0], latent_size), mean=0, stddev=1)\n",
    "    # sampling from Z~N(μ, σ^2) is the same as sampling from μ + σX, X~N(0,1)\n",
    "    return mean + K.exp(log_stddev) * std_norm\n",
    "  \n",
    "latent_vector = Lambda(sampler)([mean, log_stddev])\n",
    "# pass latent_vector as input to decoder layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 .\tUse the equations of optimal margin (linear) SVM (in pp. 12) to find w given $X_1=[1 -1]^T \\in C_{+1}$ and  $X_2=[-1 -1]^T \\in C_{-1}$  . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 .\tImplement a discrete HMM training program. Use the three-urn example (in pp. 12 of the PPT file) to test your program and produce the training results after 100 iterations. Use the red and blue balls in each urn to compute the initial emission \n",
    "probability. The initial transition probability   and  . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 .\tAssuming that the following is a part of subpixel convolution networks with stride  . Compute the resultant values with the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:02.491310Z",
     "start_time": "2018-06-18T16:44:02.488303Z"
    }
   },
   "outputs": [],
   "source": [
    "h_range, h_range = 11, 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:03.006699Z",
     "start_time": "2018-06-18T16:44:03.002688Z"
    }
   },
   "outputs": [],
   "source": [
    "k = np.array([[[3], [-1], [2]], \n",
    "               [[-2], [1], [-3]], \n",
    "               [[-2], [0], [3]]])\n",
    "p = np.zeros((13, 13, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:03.530986Z",
     "start_time": "2018-06-18T16:44:03.523968Z"
    }
   },
   "outputs": [],
   "source": [
    "p[2:-2, 2:-2, [0]] = np.array([[[6], [0], [0],  [0],  [-4], [0], [0],  [0], [1]],\n",
    "                              [[0],  [0], [0],  [0],  [0],  [0], [0],  [0], [0]],\n",
    "                              [[4],  [0], [4],  [0],  [0],  [0], [2],  [0], [1]], \n",
    "                              [[0],  [0], [0],  [0],  [0],  [0], [0],  [0], [0]],\n",
    "                              [[3],  [0], [-7], [0],  [1],  [0], [4],  [0], [2]],\n",
    "                              [[0],  [0], [0],  [0],  [0],  [0], [0],  [0], [0]],\n",
    "                              [[-2], [0], [2],  [0],  [1],  [0], [-4], [0], [2]], \n",
    "                              [[0],  [0], [0],  [0],  [0],  [0], [0],  [0], [0]],\n",
    "                              [[5],  [0], [1],  [0],  [2],  [0], [4],  [0], [-1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:04.267240Z",
     "start_time": "2018-06-18T16:44:04.264232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  6.  0.  0.  0. -4.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  4.  0.  4.  0.  0.  0.  2.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  3.  0. -7.  0.  1.  0.  4.  0.  2.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -2.  0.  2.  0.  1.  0. -4.  0.  2.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  5.  0.  1.  0.  2.  0.  4.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(p[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:06.641976Z",
     "start_time": "2018-06-18T16:44:06.639970Z"
    }
   },
   "outputs": [],
   "source": [
    "out = np.zeros((h_range, h_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:07.482022Z",
     "start_time": "2018-06-18T16:44:07.479015Z"
    }
   },
   "outputs": [],
   "source": [
    "nf = 1  # number of filters\n",
    "rf = 3  # filter size\n",
    "s = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:07.988613Z",
     "start_time": "2018-06-18T16:44:07.982597Z"
    }
   },
   "outputs": [],
   "source": [
    "for z in range(nf):\n",
    "    h_range = int((p.shape[1] - rf) / s) + 1  # (W - F + 2P) / S\n",
    "    for _h in range(h_range):\n",
    "        w_range = int((p.shape[0] - rf) / s) + 1  # (W - F + 2P) / S\n",
    "        for _w in range(w_range):\n",
    "            # print(_h, _w)\n",
    "            # print(np.sum((p[_h:_h+rf, _w:_w+rf, :]*k[:, :, :])))\n",
    "            out[_h:, _w] = np.maximum((np.sum((p[_h:_h+rf, _w:_w+rf, :]*k[:, :, :]))), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T16:44:08.694909Z",
     "start_time": "2018-06-18T16:44:08.690899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,  3.,  0.,  0.],\n",
       "       [ 0.,  6.,  0.,  0., 12.,  0.,  8.,  0.,  0.,  1.,  0.],\n",
       "       [24.,  0., 22.,  0.,  0.,  4.,  0.,  0.,  1.,  0.,  1.],\n",
       "       [ 0.,  4.,  0.,  4.,  0.,  0.,  0.,  2.,  0.,  1.,  0.],\n",
       "       [17.,  0.,  0.,  0., 29.,  0., 14.,  0.,  6.,  0.,  0.],\n",
       "       [ 0.,  3., 15.,  0., 11.,  1.,  0.,  4.,  0.,  2.,  0.],\n",
       "       [ 0.,  0.,  5.,  7.,  0.,  0.,  0.,  0., 30.,  0.,  2.],\n",
       "       [ 6.,  0.,  0.,  2.,  0.,  1., 10.,  0.,  2.,  2.,  0.],\n",
       "       [11.,  2.,  0.,  0., 12.,  0.,  3.,  4.,  0.,  0.,  8.],\n",
       "       [ 0.,  5.,  0.,  1.,  0.,  2.,  0.,  4.,  0.,  0.,  2.],\n",
       "       [10.,  0., 17.,  0.,  7.,  0., 14.,  0., 10.,  1.,  0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
